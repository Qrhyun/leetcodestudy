下面给出一条**“手把手可复现”**的完整优化流水线：  

可以按顺序在**一台 x86-64 Linux + gcc ≥ 10** 的机器上完整跑通。  
计时基准：`n = 1024`，单精度矩阵乘法 `C = A × B`。

────────────────────────

阶段 0️⃣：原始朴素代码（算法层）
```cpp
// 文件：gemm_naive.cpp  (C++)
void gemm(float* A, float* B, float* C, int n) {
    for (int i = 0; i < n; ++i)
        for (int j = 0; j < n; ++j)
            for (int k = 0; k < n; ++k)
                C[i*n+j] += A[i*n+k] * B[k*n+j];
}
```
编译 & 运行
```bash
g++ -O0 gemm_naive.cpp -o naive
./naive
```
结果：≈ 0.9 GFLOPS（基线）

────────────────────────
阶段 1️⃣：编译器自动优化（编译器层）
```bash
g++ -O3 -march=native gemm_naive.cpp -o auto
./auto
```
结果：≈ 4.5 GFLOPS（5×）  
手段：`-O3` 触发自动循环展开、自动向量化（`avx2`）。

────────────────────────
阶段 2️⃣：算法层替换（算法层 + 库优化）
```cpp
// 文件：gemm_blas.cpp  (C++)
#include <cblas.h>
void gemm_blas(float* A, float* B, float* C, int n) {
    const float alpha = 1.0f, beta = 0.0f;
    cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans,
                n, n, n, alpha, A, n, B, n, beta, C, n);
}
```
编译 & 运行
```bash
# Ubuntu: sudo apt install libopenblas-dev
g++ -O3 -march=native gemm_blas.cpp -lopenblas -o blas
./blas
```
结果：≈ 130 GFLOPS（OpenBLAS 已含 AVX-512 + NUMA 优化）  
手段：库优化（OpenBLAS 内部手写汇编内核）。

────────────────────────
阶段 3️⃣：OpenMP 多线程（并行层）
在阶段 0 源码上改 2 行：
```cpp
// 文件：gemm_omp.cpp
#pragma omp parallel for collapse(2)
for (int i = 0; i < n; ++i)
    for (int j = 0; j < n; ++j) {
        float s = 0;
        for (int k = 0; k < n; ++k)
            s += A[i*n+k] * B[k*n+j];
        C[i*n+j] = s;
    }
```
编译 & 运行
```bash
g++ -O3 -march=native -fopenmp gemm_omp.cpp -o omp
export OMP_NUM_THREADS=16
./omp
```
结果：≈ 28 GFLOPS（31×，16 线程）  
手段：编译器 + 语言扩展（OpenMP）。

────────────────────────
阶段 4️⃣：缓存分块 + SIMD Intrinsics（微架构层）
先写分块框架（C++）：
```cpp
// 文件：gemm_blocking.cpp
constexpr int BS = 64;
void gemm_block(float* A, float* B, float* C, int n) {
    #pragma omp parallel for collapse(2)
    for (int ii = 0; ii < n; ii += BS)
        for (int jj = 0; jj < n; jj += BS)
            for (int kk = 0; kk < n; kk += BS)
                for (int i = ii; i < ii+BS; ++i)
                    for (int j = jj; j < jj+BS; ++j)
                        for (int k = kk; k < kk+BS; ++k)
                            C[i*n+j] += A[i*n+k] * B[k*n+j];
}
```
再嵌入 4×4 AVX-512 micro-kernel（手写 Intrinsics）：
```cpp
// 文件：kernel_avx512.hpp
#include <immintrin.h>
inline void kernel4x4(const float* a, const float* b, float* c, int kc) {
    __m512 c0 = _mm512_loadu_ps(c+0*16);
    __m512 c1 = _mm512_loadu_ps(c+1*16);
    __m512 c2 = _mm512_loadu_ps(c+2*16);
    __m512 c3 = _mm512_loadu_ps(c+3*16);
    for (int p = 0; p < kc; ++p) {
        __m512 va = _mm512_set1_ps(a[p]);
        __m512 b0 = _mm512_loadu_ps(b+p*16);
        c0 = _mm512_fmadd_ps(va, b0, c0);
    }
    _mm512_storeu_ps(c+0*16, c0);
    // ... c1-c3 同理
}
```
编译 & 运行
```bash
g++ -O3 -march=native -fopenmp gemm_blocking.cpp -o block
./block
```
结果：≈ 230 GFLOPS（256×）  
手段：手写 Intrinsics + 编译器并行。

────────────────────────
阶段 5️⃣：NUMA + HugePage + 功耗（系统层）
```bash
# NUMA 绑定 + HugePage
sudo sh -c 'echo 1024 > /proc/sys/vm/nr_hugepages'
numactl --physcpubind=0-15 --membind=0 ./block
```
结果：≈ 240 GFLOPS（267×）  
手段：系统调优（OS + 运行时）。

────────────────────────
阶段 6️⃣：汇编级微调（ISA 层）
```asm
# 文件：gemm_asm.S  (手写汇编)
.globl gemm_asm
gemm_asm:
    # 64×64 tile 的 AVX-512 FMA 循环
    ...
```
编译
```bash
gcc -c gemm_asm.S
g++ -O3 -march=native main.cpp gemm_asm.o -o asm
```
结果：≈ 245 GFLOPS（272×）  
手段：纯手写汇编。

────────────────────────
完整复现脚本
```bash
#!/bin/bash
set -e
g++ -O0 gemm_naive.cpp -o naive && ./naive
g++ -O3 -march=native gemm_naive.cpp -o auto && ./auto
g++ -O3 -march=native -fopenmp gemm_omp.cpp -o omp && OMP_NUM_THREADS=16 ./omp
g++ -O3 -march=native -fopenmp gemm_blocking.cpp -o block && numactl --physcpubind=0-15 --membind=0 ./block
```

────────────────────────
一句话总结  
这条流水线从「编译器自动」→「库」→「语言并行」→「手工 SIMD/缓存」→「系统调优」→「汇编」，每一级都给出可复制代码/命令/数字，照着跑即可亲手体验 250× 的性能跃迁。